--------------------------------------------------------------------
---------------- Proximal Policy Optimization Agent ----------------
********************
********************
OpenAI Gym Environment used: MountainCar-v0
Environment state space size: 2
Environment action space is: discrete
Environment number of actions is: 3
Environment reward definition: Reward is -1.0 if not terminal state, 0.0 if agent has reached the flag
********************
********************
Number of training episodes: 200
Maximum iterations over one episode: 5000
Discount factor - gamma: 0.99
Clip ratio: 0.2
Actor model learning rate: 0.0003
Critic model learning rate: 0.001
Number of iterations for updating Actor model: 100
Number of iterations for updating Critic model: 100
Lambda factor: 0.97
Target KL value: 0.01
Number of units for hidden layers of Actor/Critic models: (64, 64)
Buffer size: 5000
--------------------------------------------------------------------
--------------------------------------------------------------------
Model: "Actor_Model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
dense_6 (Dense)              (None, 64)                192       
_________________________________________________________________
dense_7 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_8 (Dense)              (None, 3)                 195       
=================================================================
Total params: 4,547
Trainable params: 4,547
Non-trainable params: 0
_________________________________________________________________

--------------------------------------------------------------------
Model: "Critic_Model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
dense_9 (Dense)              (None, 64)                192       
_________________________________________________________________
dense_10 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_11 (Dense)             (None, 1)                 65        
_________________________________________________________________
tf.compat.v1.squeeze_1 (TFOp (None,)                   0         
=================================================================
Total params: 4,417
Trainable params: 4,417
Non-trainable params: 0
_________________________________________________________________

--------------------------------------------------------------------
