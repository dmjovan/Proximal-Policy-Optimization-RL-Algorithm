--------------------------------------------------------------------
---------------- Proximal Policy Optimization Agent ----------------
********************
********************
OpenAI Gym Environment used: Acrobot-v1
Environment state space size: 6
Environment action space is: discrete
Environment number of actions is: 3
Environment reward definition: Reward is -1.0 if not terminal state, 0.0 if terminal
********************
********************
Number of training episodes: 200
Maximum iterations over one episode: 5000
Discount factor - gamma: 0.99
Clip ratio: 0.2
Actor model learning rate: 0.0003
Critic model learning rate: 0.001
Number of iterations for updating Actor model: 100
Number of iterations for updating Critic model: 100
Lambda factor: 0.97
Target KL value: 0.01
Number of units for hidden layers of Actor/Critic models: (64, 64)
Buffer size: 5000
--------------------------------------------------------------------
--------------------------------------------------------------------
Model: "Actor_Model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 6)]               0         
_________________________________________________________________
dense (Dense)                (None, 64)                448       
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 195       
=================================================================
Total params: 4,803
Trainable params: 4,803
Non-trainable params: 0
_________________________________________________________________

--------------------------------------------------------------------
Model: "Critic_Model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 6)]               0         
_________________________________________________________________
dense_3 (Dense)              (None, 64)                448       
_________________________________________________________________
dense_4 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 65        
_________________________________________________________________
tf.compat.v1.squeeze (TFOpLa (None,)                   0         
=================================================================
Total params: 4,673
Trainable params: 4,673
Non-trainable params: 0
_________________________________________________________________

--------------------------------------------------------------------
